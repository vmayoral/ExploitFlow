import math
import random
import sys
import exploitflow as ef
from wasabi import color

#################################
# NOTE on network state encoding:
#################################
# Experiment designed with the following constraints:
#
# - y = 255, number of IPs considered
#     - ports
#       - n = 424, number of ports per IP considered
#       - l = 1, (open/closed) number of elements of information per port
#     - exploits
#       - m = 12, number of exploits considered
#       - b = 1, (launched/not launched) number of elements of information per exploit
#     - s = 0, system information elements considered
#
# Which leads to # of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 255 * [424*(424 + 1) + 12*(12 + 1) + 0] = 256 * 180,356 = 46,171,136
# this leads to a setup which is non-feasible computationally.
#
# Therefore, we need to reduce the number of elements considered.
# We will do so by reducing the number of ips considered. Given 6 unique IPs:
#
#   ["127.0.0.1", "192.168.2.10", "192.168.2.5", "192.168.2.6", "192.168.2.7", "192.168.2.8"]
#
# we have the following number of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 6 * [424*(424 + 1) + 12*(12 + 1) + 0] = 6 * 180,356 = 1,082,136
#
# still too many elements. We will change the number of ip and ports considered.
# We will now use the following 7 unique IPs:
#   ["127.0.0.1", "192.168.2.10", "192.168.2.5", "192.168.2.6", "192.168.2.7", "192.168.2.8", "192.168.2.1"]
# and the following ports:
#   TARGET_PORTS_BASIC = list(range(21, 30))
#
# accordingly, we have the following number of elements while encoding the state:
#   y * [n*(n + l) + m*(m + b) + s] = 7 * [9*(9 + 1) + 12*(12 + 1) + 0] = 7 * 246 = 1,722
#
# EXTRA: if we add dynamically a new exploit, we have:
#   y * [n*(n + l) + m*(m + b) + s] = 7 * [9*(9 + 1) + 13*(13 + 1) + 0] = 7 * 272 = 1,904

flow = ef.Flow()

# exploits
## initialize
init = ef.Init()
recon = ef.Targets()
versions = ef.Versions()
idle = ef.Idle()
expl = ef.adapter_msf_initializer.get_name("auxiliary", "scanner/ssh/ssh_login")
## set options
recon.target = "192.168.2.1"
versions.target = "192.168.2.10"
msf_options = {
    "RHOSTS": "192.168.2.10",
    "USERNAME": "root",
    "PASSWORD": "easybot"
}        
expl.set_options(msf_options)
expl.target = "192.168.2.10"
## set reward
idle.reward = 0
expl.reward = -10
expl.success_reward = 100
versions.reward = -10
recon.reward = -10

# ef.exploits_all = ef.exploits_all + [ef.FakeVersions2]

# global variables used for the one-hot-encoded state
exploits = [recon, versions, idle, expl, init]
# exploits = [versions, idle, expl]
exploits_encoded = [exploit.name for exploit in exploits]

# set learning model
## actions: list of actions available in the environment
## epsilon: exploration factor
## alpha: learning rate
## gamma: discount factor
flow.set_learning_model(ef.QLearn(actions=exploits_encoded, alpha=0.1, gamma=0.9, epsilon=0.1))

rollouts = 1000
episode = 10
age = 1
debug = False
last_10_actions = []

while age <= rollouts:
    
    # observe the reward and update the model
    if flow.last_state():
        flow._graph.learning_model.learn(
            tuple(flow.last_state().one_hot_encode()), 
            flow.last_action().name, 
            flow.last_reward(), 
            tuple(flow.state().one_hot_encode()),
            debug=False)
        
    # choose an action
    if flow.state():
        action = flow._graph.learning_model.chooseAction(tuple(flow.state().one_hot_encode()))
    else:
        # use init
        action = init.name  # always start with init

    last_10_actions.append(action)
    # fetch the action object from its "exploits_encoded" name
    action_expl = [exploit for exploit in exploits if exploit.name == action][0]

    # execute that action
    if flow.state():
        flow.run(flow.state() * action_expl, debug=debug)
    else:
        flow.run(action_expl, debug=debug)
    
    if age % 10 == 0:
        # debug
        age_str = "Age: " + color("{:d}".format(age), fg="white", bg="blue", bold=True)
        epsilon_str = ", epsilon: {:0.2f}".format(flow._graph.learning_model.epsilon)
        reward_str = ", reward: " + color("{:0.2f}".format(flow.reward()), fg="white", bg="red", bold=True)        
        bytes = sys.getsizeof(flow._graph.learning_model.q)
        q_size_str = ", Q-size bytes: {:d} ".format(bytes)
        q_size_str += color("({:.2f} KB, {:d} elements)".format(bytes/1024, len(flow._graph.learning_model.q.keys())), 
                            fg="white", bg="blue", bold=True)
        print_message = age_str + epsilon_str + reward_str + q_size_str
    
        # last_actions_str = " -- last 10 actions: " + ", " + color([str(n) for n in last_10_actions[-10:]], fg="black", bg="white")
        # last_10_actions = []        
        # print_message += last_actions_str
        print(print_message)

    if age == rollouts:
        print(flow)

    if age % episode == 0:
        # reset the flow
        flow.reset()  #Â reward=0, state=None, last_state=None, last_action=None, last_reward=None

    # next rollout
    age += 1
